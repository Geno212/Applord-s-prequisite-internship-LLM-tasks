{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e25dc48-1eae-40bf-ba70-4e229fbc96a1",
   "metadata": {},
   "source": [
    "Implement for all llms(token + get memory usage function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe79ca6-f395-43ce-9fc7-adf526eae0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "token = \"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\"\n",
    "HfFolder.save_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d5d87-f4ea-47fb-a0ad-425500feb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm-eval --tasks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c90b12-8526-45c7-95a7-99c7116a6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-2,dtype=\"float16\" \\\n",
    "    --tasks hellaswag \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size 6\\\n",
    "    --output_path ./results \\\n",
    "    --log_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "161522c6-7011-45ea-9c3d-c185060048f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Function to get memory usage in MBG\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory_usage = process.memory_info().rss / 1024 / 1024  # in MB\n",
    "    return memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd50e8e3-140c-4c72-9574-5010a46768b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Time: 2.011967420578003 seconds\n",
      "Memory Used before loading: 385.71 MB\n",
      "Memory Used after loading: 823.77 MB\n",
      "Memory Used for loading: 438.06 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import time\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "m1=get_memory_usage()\n",
    "startload_time = time.time()\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "endload_time = time.time()\n",
    "m2=get_memory_usage()\n",
    "\n",
    "memory_loading=m2 - m1\n",
    "load_time=endload_time-startload_time\n",
    "\n",
    "print(f\"loading Time: {load_time} seconds\")\n",
    "print(f\"Memory Used before loading: {m1:.2f} MB\")\n",
    "print(f\"Memory Used after loading: {m2:.2f} MB\")\n",
    "print(f\"Memory Used for loading: {memory_loading:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9bf1b8-5b6f-4e8d-938a-8f103c70884b",
   "metadata": {},
   "source": [
    "microsoft/phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b177812-6d04-4a93-ab51-5c74265e2ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_11704\\4059773454.py:55: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('bleu', trust_remote_code=True)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      "progr√®s rapides dans l'intelligence artificial ont transform√© de nombreux secteurs, en rendre les processus plus efficaces et en encouragant des nouvelles capacit√©s qui √©taient ant√©rieurs inimaginables. ¬ª\n",
      "\n",
      "Reference:\n",
      "[\"Les avanc√©es rapides de l'intelligence artificielle ont transform√© de nombreuses industries, rendant les processus plus efficaces et permettant de nouvelles capacit√©s auparavant inimaginables.\", \"Les progr√®s rapides de l'intelligence artificielle ont r√©volutionn√© de nombreux secteurs, rendant les processus plus efficients et ouvrant de nouvelles possibilit√©s autrefois inimaginables.\", \"Les d√©veloppements rapides de l'intelligence artificielle ont chang√© de nombreuses industries, rendant les op√©rations plus efficaces et permettant des capacit√©s nouvelles autrefois inimaginables.\", \"Les progr√®s rapides dans le domaine de l'intelligence artificielle ont modifi√© de nombreuses industries, rendant les processus plus efficients et permettant des capacit√©s nouvelles autrefois inimaginables.\"]\n",
      "\n",
      "BLEU Score: {'bleu': 0.3128418715354195, 'precisions': [0.6896551724137931, 0.4642857142857143, 0.25925925925925924, 0.11538461538461539], 'brevity_penalty': 1.0, 'length_ratio': 1.16, 'translation_length': 29, 'reference_length': 25}\n",
      "meteor Score: {'meteor': 0.15419819875853547}\n",
      "Inference Time: 7.994086980819702 seconds\n",
      "Memory Usage Before Inference: 3467.14 MB\n",
      "Memory Usage After Inference: 3510.94 MB\n",
      "Memory Used: 43.80 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=150, num_return_sequences=1, temperature=0.9, top_p=0.9, top_k=50)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "    \n",
    "# Function to extract answer from prompt\n",
    "def extract_answer(response):\n",
    "    # Find the position of \"A:\" in the prompt\n",
    "    start_idx = response.find(\"A:\") + 7\n",
    "    # Return the answer part after \"A:\"\n",
    "    return response[start_idx:].strip()\n",
    "\n",
    "# Define your prompt for evaluation\n",
    "prompt = \"Translate the following English sentence to French with only one sentence: 'The rapid advancements in artificial intelligence have transformed many industries, making processes more efficient and enabling new capabilities that were previously unimaginable.' Answer:\"\n",
    "references =  [\n",
    "    \"Les avanc√©es rapides de l'intelligence artificielle ont transform√© de nombreuses industries, rendant les processus plus efficaces et permettant de nouvelles capacit√©s auparavant inimaginables.\",\n",
    "    \"Les progr√®s rapides de l'intelligence artificielle ont r√©volutionn√© de nombreux secteurs, rendant les processus plus efficients et ouvrant de nouvelles possibilit√©s autrefois inimaginables.\",\n",
    "    \"Les d√©veloppements rapides de l'intelligence artificielle ont chang√© de nombreuses industries, rendant les op√©rations plus efficaces et permettant des capacit√©s nouvelles autrefois inimaginables.\",\n",
    "    \"Les progr√®s rapides dans le domaine de l'intelligence artificielle ont modifi√© de nombreuses industries, rendant les processus plus efficients et permettant des capacit√©s nouvelles autrefois inimaginables.\"\n",
    "]\n",
    "\n",
    "# Measure memory usage before inference\n",
    "memory_before = get_memory_usage()\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate model output\n",
    "output =generate_response(prompt)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "# Measure memory usage after inference\n",
    "memory_after = get_memory_usage()\n",
    "\n",
    "# Calculate inference time and load times\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Calculate memory usage\n",
    "memory_used = memory_after - memory_before\n",
    "\n",
    "# Extract the generated output\n",
    "generated_output = extract_answer(output)\n",
    "tokenized_generated_output = word_tokenize(generated_output)\n",
    "# Tokenize references\n",
    "tokenized_references = [word_tokenize(ref) for ref in references]\n",
    "\n",
    "# Initialize the metric\n",
    "metric = load_metric('bleu', trust_remote_code=True)\n",
    "metric.add(prediction=tokenized_generated_output, reference=tokenized_references)\n",
    "\n",
    "metricmeteor=load_metric(\"meteor\", trust_remote_code=True)\n",
    "metricmeteor.add(prediction=tokenized_generated_output, reference=tokenized_references)\n",
    "\n",
    "# Compute the final BLEU score\n",
    "final_score = metric.compute()\n",
    "met_score=metricmeteor.compute()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Generated Output:\\n{generated_output}\\n\")\n",
    "print(f\"Reference:\\n{references}\\n\")\n",
    "print(f\"BLEU Score: {final_score}\")\n",
    "print(f\"meteor Score: {met_score}\")\n",
    "print(f\"Inference Time: {inference_time} seconds\")\n",
    "print(f\"Memory Usage Before Inference: {memory_before:.2f} MB\")\n",
    "print(f\"Memory Usage After Inference: {memory_after:.2f} MB\")\n",
    "print(f\"Memory Used: {memory_used:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdedab90-2540-47fe-af9f-4331c1d5d5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      "progr√®s rapides dans l'intelligence artificial ont transform√© de nombreux secteurs, en rendre les processus plus efficaces et en encouragant des nouvelles capacit√©s qui √©taient ant√©rieurs inimaginables. ¬ª\n",
      "\n",
      "References:\n",
      "[\"Les avanc√©es rapides de l'intelligence artificielle ont transform√© de nombreuses industries, rendant les processus plus efficaces et permettant de nouvelles capacit√©s auparavant inimaginables.\", \"Les progr√®s rapides de l'intelligence artificielle ont r√©volutionn√© de nombreux secteurs, rendant les processus plus efficients et ouvrant de nouvelles possibilit√©s autrefois inimaginables.\", \"Les d√©veloppements rapides de l'intelligence artificielle ont chang√© de nombreuses industries, rendant les op√©rations plus efficaces et permettant des capacit√©s nouvelles autrefois inimaginables.\", \"Les progr√®s rapides dans le domaine de l'intelligence artificielle ont modifi√© de nombreuses industries, rendant les processus plus efficients et permettant des capacit√©s nouvelles autrefois inimaginables.\"]\n",
      "\n",
      "Precision: 0.5172\n",
      "Recall: 0.5829\n",
      "F1 Score: 0.5478\n",
      "Accuracy: 0.5829\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "from datasets import load_metric\n",
    "from nltk.tokenize import word_tokenize\n",
    "import psutil\n",
    "from collections import Counter\n",
    "\n",
    "# Mapping tokens to integer IDs\n",
    "vocab = {word: idx for idx, word in enumerate(set(tokenized_generated_output + [word for ref in tokenized_references for word in ref]))}\n",
    "int_generated_output = [vocab[word] for word in tokenized_generated_output]\n",
    "int_references = [[vocab[word] for word in ref] for ref in tokenized_references]\n",
    "\n",
    "# Calculate precision, recall, f1, and accuracy\n",
    "def compute_metrics(predictions, references):\n",
    "    pred_counter = Counter(predictions)\n",
    "    ref_counter = Counter(references)\n",
    "\n",
    "    tp = sum((pred_counter & ref_counter).values())\n",
    "    fp = len(predictions) - tp\n",
    "    fn = len(references) - tp\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    accuracy = tp / len(references) if len(references) > 0 else 0.0\n",
    "\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for ref in int_references:\n",
    "    precision, recall, f1, accuracy = compute_metrics(int_generated_output, ref)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Average the scores across all references\n",
    "precision_score = sum(precision_scores) / len(precision_scores)\n",
    "recall_score = sum(recall_scores) / len(recall_scores)\n",
    "f1_score = sum(f1_scores) / len(f1_scores)\n",
    "accuracy_score = sum(accuracy_scores) / len(accuracy_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Generated Output:\\n{generated_output}\\n\")\n",
    "print(f\"References:\\n{references}\\n\")\n",
    "print(f\"Precision: {precision_score:.4f}\")\n",
    "print(f\"Recall: {recall_score:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2516e124-ee4d-41df-820f-be4e34d94e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: The following article is a summary of the research and development of Artificial Intelligence (AI) in the field of healthcare. It is based on the work of the University of California, San Francisco, and the University of California, Berkeley. The research was funded by the National Science Foundation (NSF) and the National Science Foundation (NSF). The research was conducted by the University of California, San Francisco, and the University of California, Berkeley\n",
      "reference: AI has rapidly advanced, transforming industries by improving efficiencies and creating new capabilities.\n",
      "ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765), mid=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765), high=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765), mid=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765), high=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765)), 'rougeLsum': AggregateScore(low=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765), mid=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765), high=Score(precision=0.041666666666666664, recall=0.23076923076923078, fmeasure=0.07058823529411765))}\n"
     ]
    }
   ],
   "source": [
    "promptrouge = \"Summarize the following article in one sentence: 'Artificial intelligence (AI) has seen rapid advancements in recent years, transforming various industries. It has improved efficiencies, created new capabilities, and reshaped the workforce. AI's impact is far-reaching, influencing sectors from healthcare to finance.' A:\"\n",
    "reference_summary = \"AI has rapidly advanced, transforming industries by improving efficiencies and creating new capabilities.\"\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=150, num_return_sequences=1, temperature=0.7, top_p=0.9)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "# Function to extract answer from prompt\n",
    "def extract_answer2(response):\n",
    "    # Find the position of \"A:\" in the prompt\n",
    "    start_idx = response.find(\"A:\") + 2\n",
    "    # Return the answer part after \"A:\"\n",
    "    return response[start_idx:].strip()\n",
    "    \n",
    "output2 = generate_response(promptrouge)\n",
    "generated_output2 = extract_answer2(output2)\n",
    "\n",
    "# Initialize the metric\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "# Add predictions and references to the metric\n",
    "rouge.add(prediction=generated_output2, reference=reference_summary)\n",
    "\n",
    "# Compute the final ROUGE score\n",
    "final_score = rouge.compute()\n",
    "print(f\"output: {generated_output2}\")\n",
    "print(f\"reference: {reference_summary}\")\n",
    "print(f\"ROUGE Score: {final_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d32170f-ecf9-4c71-8605-e0c91bca0dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sentiment classification: I got a promotion at work and my colleagues threw a party for me. I am thrilled.', 99.22355075451298), ('sentiment classification: I got a promotion at work and my colleagues threw a party for me. I am disappointed.', 256.0163618454558)]\n",
      "Prediction: sentiment classification: I got a promotion at work and my colleagues threw a party for me. I am thrilled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import lmppl\n",
    "\n",
    "scorer = lmppl.EncoderDecoderLM('google/flan-t5-large')\n",
    "inputs = [\n",
    "    \"sentiment classification: I got a promotion at work and my colleagues threw a party for me.\",\n",
    "    \"sentiment classification: I got a promotion at work and my colleagues threw a party for me.\"\n",
    "]\n",
    "outputs=[\"I am thrilled.\", \"I am disappointed.\"]\n",
    "ppl = scorer.get_perplexity(input_texts=inputs, output_texts=outputs)\n",
    "print(list(zip(texts, ppl)))\n",
    "print(f\"Prediction: {texts[ppl.index(min(ppl))]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1dcfc8ac-1822-4c36-a655-87986868dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 256.21 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to your Llama model\n",
    "model_path = r\"C:\\Users\\acer\\.cache\\huggingface\\hub\\models--distilbert-base-uncased\"\n",
    "\n",
    "# Function to get size of a file or directory\n",
    "def get_model_size(path):\n",
    "    # Check if path is a file\n",
    "    if os.path.isfile(path):\n",
    "        return os.path.getsize(path) / (1024 * 1024)  # in MB\n",
    "    # Check if path is a directory\n",
    "    elif os.path.isdir(path):\n",
    "        total_size = 0\n",
    "        for dirpath, _, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                total_size += os.path.getsize(filepath)\n",
    "        return total_size / (1024 * 1024)  # in MB\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is not a valid file or directory.\")\n",
    "\n",
    "# Get size of the Llama model\n",
    "model_size = get_model_size(model_path)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model Size: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776f6f7-a78b-494a-b58d-51aaaf99c065",
   "metadata": {},
   "source": [
    "tiiuae/falcon-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8460e4e-f142-45ea-8d98-6c92b5a58669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sacrebleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "import torch\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=200, num_return_sequences=1, temperature=0.7, top_p=0.9)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Define your prompt for evaluation\n",
    "prompt = \"Q: tell me a story about a brave knight A: \"\n",
    "# Measure memory usage before inference\n",
    "memory_before = get_memory_usage()\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "# Generate model output\n",
    "output=generate_response(prompt)\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "# Measure memory usage after inference\n",
    "memory_after = get_memory_usage()\n",
    "# Calculate inference time\n",
    "inference_time = end_time - start_time\n",
    "memory_used = memory_after - memory_before\n",
    "# Extract generated text from the output\n",
    "generated_output =output\n",
    "\n",
    "# Reference text (ground truth) for evaluation\n",
    "reference = \"A brave knight fought valiantly against the dragon.\"\n",
    "\n",
    "# Tokenize generated output and reference\n",
    "generated_tokens = word_tokenize(generated_output)\n",
    "reference_tokens = word_tokenize(reference)\n",
    "\n",
    "# Compute BLEU score using SacreBLEU\n",
    "bleu = sacrebleu.corpus_bleu([generated_output], [[reference]])\n",
    "\n",
    "# Print the generated output and BLEU score\n",
    "print(f\"Generated Output:\\n{generated_output}\\n\")\n",
    "print(f\"Reference:\\n{reference}\\n\")\n",
    "print(f\"BLEU Score: {bleu.score}\")\n",
    "print(f\"Inference Time: {inference_time} seconds\")\n",
    "# Print memory usage\n",
    "print(f\"Memory Usage Before Inference: {memory_before:.2f} MB\")\n",
    "print(f\"Memory Usage After Inference: {memory_after:.2f} MB\")\n",
    "print(f\"Memory Used: {memory_used:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75ef88-920d-4ad0-825c-d01382be279a",
   "metadata": {},
   "source": [
    "bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18b60bea-c86a-4842-9c05-3a898cfe5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input:\n",
      " {'input_ids': tensor([[  101,  7976,  4454,  2038,  6022, 19209,  2536,  6088,  1010, 20226,\n",
      "          6194,  1998, 12067,  2047,  9859,  2008,  2020,  3130,  4895,  9581,\n",
      "         20876,  3468,  1012,  1999,  1996,  2492,  1997,  9871,  1010,  9932,\n",
      "          2038,  4329,  3550,  5776, 11616,  1998,  3949,  4041,  1010,   103,\n",
      "         12607,  2015,  1999,  2966, 12126,  1998,  3167,  3550,  4200,  1012,\n",
      "          7297,  1010,  9932,  1011,  5533,  6786,  2031,  2042,  4162,  2000,\n",
      "           103, 11105,  2107,  2004,  5446,  1010, 23569, 27605,  6774,  6202,\n",
      "          9942,  1998,  9861, 10788, 13792,  1012,  2174,  1010, 12962,  5936,\n",
      "           103,  2004,  2951,  9394,  1998,  9896,  2594, 13827,  3613,  2000,\n",
      "          4119,  6923,  9932,  9886,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Mask Token Indices:\n",
      " [[38, 59, 79]]\n",
      "\n",
      "Generated Summaries:\n",
      "- including other such\n",
      "\n",
      "References:\n",
      "- AI advancements in medical imaging and personalized medicine have transformed healthcare.\n",
      "- AI applications in finance have optimized trading strategies and enhanced fraud detection.\n",
      "- Ethical concerns such as data privacy and algorithmic bias remain challenges for AI adoption.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example text with masked tokens and references\n",
    "text = [\n",
    "    \"Artificial intelligence has significantly impacted various industries, enhancing processes and enabling new capabilities that were previously unimaginable. \" \\\n",
    "    \"In the field of healthcare, AI has revolutionized patient diagnosis and treatment planning, [MASK] advancements in medical imaging and personalized medicine. \" \\\n",
    "    \"Furthermore, AI-driven technologies have been applied to [MASK] sectors such as finance, optimizing trading strategies and fraud detection algorithms. \" \\\n",
    "    \"However, ethical concerns [MASK] as data privacy and algorithmic bias continue to challenge widespread AI adoption.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"AI advancements in medical imaging and personalized medicine have transformed healthcare.\",\n",
    "    \"AI applications in finance have optimized trading strategies and enhanced fraud detection.\",\n",
    "    \"Ethical concerns such as data privacy and algorithmic bias remain challenges for AI adoption.\"\n",
    "]\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_input = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Mask token indices within each sentence\n",
    "mask_token_indices = []\n",
    "for i, sentence in enumerate(text):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    mask_indices = [j for j, token in enumerate(tokens) if token == '[MASK]']\n",
    "    mask_token_indices.append(mask_indices)\n",
    "\n",
    "# Print tokenized input and mask token indices for debugging\n",
    "print(\"Tokenized Input:\\n\", tokenized_input)\n",
    "print(\"\\nMask Token Indices:\\n\", mask_token_indices)\n",
    "\n",
    "# Predict for each masked token\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_input)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Generate summaries\n",
    "generated_summaries = []\n",
    "for i, (sentence, indices) in enumerate(zip(text, mask_token_indices)):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    predicted_tokens = []\n",
    "    for idx in indices:\n",
    "        predicted_token_id = torch.argmax(predictions[i, idx + 1]).item()  # Skip the [CLS] token\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        predicted_tokens.append(predicted_token)\n",
    "    generated_summary = \" \".join(predicted_tokens)\n",
    "    generated_summaries.append(generated_summary)\n",
    "\n",
    "# Print generated summaries and references\n",
    "print(\"\\nGenerated Summaries:\")\n",
    "for summary in generated_summaries:\n",
    "    print(\"-\", summary)\n",
    "\n",
    "print(\"\\nReferences:\")\n",
    "for ref in references:\n",
    "    print(\"-\", ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fb97b7e1-7540-4187-8a13-c6d818cf0a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The man went to school every day because he worked as a [MASK].\n",
      "Reference: The man went to school every day because he worked as a teacher.\n",
      "Prediction: the man went to school every day because he worked as alating.\n",
      "\n",
      "Original: The woman went to the hospital every day because she worked as a [MASK].\n",
      "Reference: The woman went to the hospital every day because she worked as a nurse.\n",
      "Prediction: the woman went to the hospital every day because she worked as a sean.\n",
      "\n",
      "BLEU score: 0.793137238008071\n",
      "METEOR score: 0.8966418701773274\n",
      "Inference Time: 4.60369610786438 seconds\n",
      "Memory Usage Before Inference: 2991.66 MB\n",
      "Memory Usage After Inference: 2995.76 MB\n",
      "Memory Used: 4.10 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "import time\n",
    "# Measure memory usage before inference\n",
    "memory_before = get_memory_usage()\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "model.eval()  # Put the model in evaluation mode\n",
    "\n",
    "# Example sentences with masked tokens and their references\n",
    "sentences = [\n",
    "    \"The man went to school every day because he worked as a [MASK].\",\n",
    "    \"The woman went to the hospital every day because she worked as a [MASK].\"\n",
    "]\n",
    "references = [\n",
    "    [\"The man went to school every day because he worked as a teacher.\"],\n",
    "    [\"The woman went to the hospital every day because she worked as a nurse.\"]\n",
    "]\n",
    "\n",
    "# Tokenize input sentences\n",
    "tokenized_inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass, get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_inputs)\n",
    "\n",
    "# Get the predictions for masked tokens\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Decode predictions\n",
    "predicted_sentences = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    masked_index = tokenized_inputs['input_ids'][i].tolist().index(tokenizer.mask_token_id)\n",
    "    input_ids = tokenized_inputs['input_ids'][i].tolist()\n",
    "    input_ids[masked_index] = predictions[i, masked_index].item()\n",
    "    predicted_sentence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    predicted_sentences.append(predicted_sentence)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Reference: {references[i][0]}\")\n",
    "    print(f\"Prediction: {predicted_sentence}\")\n",
    "    print()\n",
    "    \n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "# Measure memory usage after inference\n",
    "memory_after = get_memory_usage()\n",
    "# Calculate inference time\n",
    "inference_time = end_time - start_time\n",
    "memory_used = memory_after - memory_before\n",
    "\n",
    "# Load metrics\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "meteor_metric = load_metric(\"meteor\")\n",
    "\n",
    "# Prepare references and predictions for metric calculation\n",
    "bleu_references = [[[word for word in ref[0].split()]] for ref in references]\n",
    "bleu_predictions = [[word for word in pred.split()] for pred in predicted_sentences]\n",
    "meteor_references = [ref[0] for ref in references]\n",
    "meteor_predictions = predicted_sentences\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_metric.add_batch(predictions=bleu_predictions, references=bleu_references)\n",
    "bleu_score = bleu_metric.compute()\n",
    "print(f\"BLEU score: {bleu_score['bleu']}\")\n",
    "\n",
    "# Calculate METEOR score\n",
    "meteor_metric.add_batch(predictions=meteor_predictions, references=meteor_references)\n",
    "meteor_score = meteor_metric.compute()\n",
    "print(f\"METEOR score: {meteor_score['meteor']}\")\n",
    "print(f\"Inference Time: {inference_time} seconds\")\n",
    "# Print memory usage\n",
    "print(f\"Memory Usage Before Inference: {memory_before:.2f} MB\")\n",
    "print(f\"Memory Usage After Inference: {memory_after:.2f} MB\")\n",
    "print(f\"Memory Used: {memory_used:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "32716528-803b-4d81-81a1-b24042f40f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.sa_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Positive\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Assuming binary classification\n",
    "\n",
    "def classify_sequence(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "text = \"I love this product!.\"\n",
    "predicted_class = classify_sequence(text)\n",
    "print(\"Predicted Class:\", \"Positive\" if predicted_class == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981cc40-35d9-4b24-b624-20d871738229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450836e-f203-4e3a-b94d-3c2fb25ac465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model for question answering\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def generate_response(context, question):\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits) + 1\n",
    "    \n",
    "    answer_tokens = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Define your context and question for evaluation\n",
    "context = \"\"\"\n",
    "Forbes estimates Elon Musk net worth to be US$221 billion.\n",
    "\"\"\"\n",
    "question = \"How much money is Elon Musk's net worth?\"\n",
    "\n",
    "# Generate model output\n",
    "output = generate_response(context, question)\n",
    "\n",
    "# Print the question and answer\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad069f-e053-4029-a27a-7bba9d7edbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b6e51-a997-486a-a489-2cc5ce2e072a",
   "metadata": {},
   "source": [
    "meta-llama/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f61b6-1422-4251-ba2f-321dda98e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=200, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5c1b0-d6f5-460e-9c09-2520ee7862ab",
   "metadata": {},
   "source": [
    "mistralai/Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609572d-627f-4d4d-b482-c3ba10492f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "mistralmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  \n",
    "tokenizer2 = AutoTokenizer.from_pretrained(mistralmodel_name,token=token)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(mistralmodel_name,token=token)\n",
    "def generate_mistral_response(prompt):\n",
    "    inputs2 = tokenizer2(prompt, return_tensors=\"pt\")\n",
    "    outputs2 = model2.generate(inputs2.input_ids, max_length=200, num_return_sequences=1)\n",
    "    response2 = tokenizer2.decode(outputs2[0], skip_special_tokens=True)\n",
    "    return response2\n",
    "promptt = \"tell me a story about a mysterious island\"\n",
    "response2 = generate_mistral_response(promptt)\n",
    "\n",
    "print(response2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046ca87-a0da-4411-953e-efc3f395ab89",
   "metadata": {},
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa27eb3-b01c-404f-8181-c3ac8420295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"gpt2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "model.eval()\n",
    "def generate_mistral_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=50, num_return_sequences=1, temperature=0.7, top_p=0.9)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "prompt = \"define the word engineering\"\n",
    "response = generate_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9376d5-879e-422e-96f8-811eeeb5cf8c",
   "metadata": {},
   "source": [
    "distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c03d0-c2a4-4fc8-ac09-dc6b7f167b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Assuming binary classification\n",
    "\n",
    "def classify_sequence(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "text = \"I love this product! It works great and is exactly what I needed.\"\n",
    "predicted_class = classify_sequence(text)\n",
    "print(\"Predicted Class:\", \"Positive\" if predicted_class == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb336e-90e7-4dba-9799-7c38dfb6ebd1",
   "metadata": {},
   "source": [
    "google/flan-t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4074ae-93d6-46af-aa66-f6d40814d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name,token=token)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name,token=token)\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, max_length=500, num_return_sequences=1, temperature=0.8, top_p=0.9, do_sample=True)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5693b-4998-4880-b8f4-68085eb8fe86",
   "metadata": {},
   "source": [
    "nghuyong/ernie-2.0-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b7d6b-c124-4f75-8b85-145042c9c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, ErnieForCausalLM\n",
    "\n",
    "model_name = \"nghuyong/ernie-2.0-en\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = ErnieForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids, \n",
    "                             max_length=50, \n",
    "                             num_return_sequences=1, \n",
    "                             temperature=0.8,  # Adjust temperature for randomness\n",
    "                             top_p=0.9          # Adjust top_p for diversity\n",
    "                            )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "prompt = \"define the word engineering\"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f4ccb-77bc-4fb1-8355-bdc7f960a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a639a5-b40a-42d4-b1c3-8f55bfd82150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Function to get memory usage in MB\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory_usage = process.memory_info().rss / 1024 / 1024  # in MB\n",
    "    return memory_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658d7f57-a020-4620-bcd4-1e244b1c2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from C:\\Users\\acer\\Documents\\zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'general.name': 'huggingfaceh4_zephyr-7b-beta', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =    3901.87 ms\n",
      "llama_print_timings:      sample time =      62.03 ms /   100 runs   (    0.62 ms per token,  1612.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3901.77 ms /    15 tokens (  260.12 ms per token,     3.84 tokens per second)\n",
      "llama_print_timings:        eval time =   32784.76 ms /    99 runs   (  331.16 ms per token,     3.02 tokens per second)\n",
      "llama_print_timings:       total time =   36906.25 ms /   114 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      "Q: tell me a story about a brave knight A:  Once upon a time, in a land far away, there was a brave knight named Sir Alexander. He was tall and strong with a heart full of courage and determination. His kingdom was threatened by an evil sorcerer who had cast a dark spell on the land, turning the fields barren and the skies gray. The people were suffering and Sir Alexander knew he had to act. With his trusty steed by his side, he set off on a dangerous quest to defeat the sor\n",
      "\n",
      "Reference:\n",
      "A brave knight fought valiantly against the dragon.\n",
      "\n",
      "BLEU Score: 0.859122307088785\n",
      "Inference Time: 36.90883660316467 seconds\n",
      "Memory Usage Before Inference: 273.73 MB\n",
      "Memory Usage After Inference: 4385.82 MB\n",
      "Memory Used: 4112.09 MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from llama_cpp import Llama\n",
    "import sacrebleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the Llama model with the correct path to your model\n",
    "llm = Llama(\n",
    "    model_path=r\"C:\\Users\\acer\\Documents\\zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "    # You can uncomment and adjust parameters like n_gpu_layers, seed, or n_ctx here if needed\n",
    ")\n",
    "\n",
    "# Define your prompt for evaluation\n",
    "prompt = \"Q: tell me a story about a brave knight A: \"\n",
    "\n",
    "# Measure memory usage before inference\n",
    "memory_before = get_memory_usage()\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "# Generate model output\n",
    "output = llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=100,  # Generate up to 100 tokens\n",
    "    stop=[\"Q:\", \"\\n\"],  # Stop generating before the model generates a new question or newline\n",
    "    echo=True,  # Echo the prompt back in the output\n",
    "    temperature=0.7,  # Adjust temperature for randomness (0.7 is moderately random)\n",
    "    top_p=0.9  # Adjust top_p for diversity (0.9 means considering the top 90% of probability mass)\n",
    ")\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "# Measure memory usage after inference\n",
    "memory_after = get_memory_usage()\n",
    "# Calculate inference time\n",
    "inference_time = end_time - start_time\n",
    "memory_used = memory_after - memory_before\n",
    "# Extract generated text from the output\n",
    "generated_output = output['choices'][0]['text']\n",
    "\n",
    "# Reference text (ground truth) for evaluation\n",
    "reference = \"A brave knight fought valiantly against the dragon.\"\n",
    "\n",
    "# Tokenize generated output and reference\n",
    "generated_tokens = word_tokenize(generated_output)\n",
    "reference_tokens = word_tokenize(reference)\n",
    "\n",
    "# Compute BLEU score using SacreBLEU\n",
    "bleu = sacrebleu.corpus_bleu([generated_output], [[reference]])\n",
    "\n",
    "# Print the generated output and BLEU score\n",
    "print(f\"Generated Output:\\n{generated_output}\\n\")\n",
    "print(f\"Reference:\\n{reference}\\n\")\n",
    "print(f\"BLEU Score: {bleu.score}\")\n",
    "print(f\"Inference Time: {inference_time} seconds\")\n",
    "# Print memory usage\n",
    "print(f\"Memory Usage Before Inference: {memory_before:.2f} MB\")\n",
    "print(f\"Memory Usage After Inference: {memory_after:.2f} MB\")\n",
    "print(f\"Memory Used: {memory_used:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f28b1a3-e2ee-4303-a184-6803f35dd76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "rouge-1: 0.10126582096458903\n",
      "rouge-2: 0.01923076797522198\n",
      "rouge-l: 0.10126582096458903\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# Convert tokens back to strings for ROUGE evaluation\n",
    "generated_text = ' '.join(generated_tokens)\n",
    "reference_text = ' '.join(reference_tokens)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_scores = rouge.get_scores(generated_text, reference_text)\n",
    "\n",
    "# Print ROUGE scores\n",
    "print(f\"ROUGE Scores:\")\n",
    "for metric, scores in rouge_scores[0].items():\n",
    "    print(f\"{metric}: {scores['f']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a1a2f8-065c-498c-bc96-71f539089210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 4166.07 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to your Llama model\n",
    "model_path = r\"C:\\Users\\acer\\Documents\\zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "\n",
    "# Function to get size of a file or directory\n",
    "def get_model_size(path):\n",
    "    # Check if path is a file\n",
    "    if os.path.isfile(path):\n",
    "        return os.path.getsize(path) / (1024 * 1024)  # in MB\n",
    "    # Check if path is a directory\n",
    "    elif os.path.isdir(path):\n",
    "        total_size = 0\n",
    "        for dirpath, _, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                total_size += os.path.getsize(filepath)\n",
    "        return total_size / (1024 * 1024)  # in MB\n",
    "    else:\n",
    "        raise ValueError(f\"Path '{path}' is not a valid file or directory.\")\n",
    "\n",
    "# Get size of the Llama model\n",
    "model_size = get_model_size(model_path)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model Size: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c0e0ebe-1021-435b-b3d8-445f7fc50c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: saved_model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 4096\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 11008\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
      "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 2\n",
      "INFO:gguf.vocab:Setting special token type unk to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F16, shape = {4096, 32000}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00006.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.float32 --> F16, shape = {4096, 32000}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float32 --> F16, shape = {11008, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float32 --> F16, shape = {4096, 11008}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float32 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float32 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {4096}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:llama2_7b_chat.gguf: n_tensors = 291, total_size = 13.5G\n",
      "\n",
      "Writing:   0%|          | 0.00/13.5G [00:00<?, ?byte/s]\n",
      "Writing:   2%|1         | 262M/13.5G [00:01<01:06, 199Mbyte/s]\n",
      "Writing:   3%|2         | 352M/13.5G [00:01<01:07, 195Mbyte/s]\n",
      "Writing:   3%|3         | 443M/13.5G [00:02<01:07, 193Mbyte/s]\n",
      "Writing:   4%|3         | 533M/13.5G [00:03<01:26, 150Mbyte/s]\n",
      "Writing:   4%|4         | 566M/13.5G [00:03<01:40, 128Mbyte/s]\n",
      "Writing:   4%|4         | 600M/13.5G [00:03<01:45, 122Mbyte/s]\n",
      "Writing:   5%|4         | 633M/13.5G [00:04<01:58, 108Mbyte/s]\n",
      "Writing:   5%|4         | 667M/13.5G [00:04<01:59, 107Mbyte/s]\n",
      "Writing:   6%|5         | 757M/13.5G [00:05<02:01, 105Mbyte/s]\n",
      "Writing:   6%|6         | 847M/13.5G [00:06<02:02, 103Mbyte/s]\n",
      "Writing:   7%|6         | 937M/13.5G [00:07<02:02, 102Mbyte/s]\n",
      "Writing:   7%|7         | 971M/13.5G [00:07<02:08, 97.2Mbyte/s]\n",
      "Writing:   7%|7         | 1.00G/13.5G [00:08<02:09, 96.5Mbyte/s]\n",
      "Writing:   8%|7         | 1.04G/13.5G [00:08<02:17, 90.4Mbyte/s]\n",
      "Writing:   8%|7         | 1.07G/13.5G [00:09<02:14, 92.3Mbyte/s]\n",
      "Writing:   9%|8         | 1.16G/13.5G [00:10<02:12, 93.0Mbyte/s]\n",
      "Writing:   9%|9         | 1.25G/13.5G [00:10<02:11, 93.3Mbyte/s]\n",
      "Writing:  10%|9         | 1.34G/13.5G [00:11<02:10, 93.0Mbyte/s]\n",
      "Writing:  10%|#         | 1.38G/13.5G [00:12<02:14, 90.3Mbyte/s]\n",
      "Writing:  10%|#         | 1.41G/13.5G [00:12<02:12, 90.8Mbyte/s]\n",
      "Writing:  11%|#         | 1.44G/13.5G [00:13<02:17, 87.3Mbyte/s]\n",
      "Writing:  11%|#         | 1.48G/13.5G [00:13<02:18, 86.7Mbyte/s]\n",
      "Writing:  12%|#1        | 1.57G/13.5G [00:14<02:12, 89.8Mbyte/s]\n",
      "Writing:  12%|#2        | 1.66G/13.5G [00:15<02:11, 90.2Mbyte/s]\n",
      "Writing:  13%|#2        | 1.75G/13.5G [00:16<02:09, 90.3Mbyte/s]\n",
      "Writing:  13%|#3        | 1.78G/13.5G [00:16<02:13, 87.3Mbyte/s]\n",
      "Writing:  13%|#3        | 1.81G/13.5G [00:17<02:10, 89.3Mbyte/s]\n",
      "Writing:  14%|#3        | 1.85G/13.5G [00:17<02:24, 80.4Mbyte/s]\n",
      "Writing:  14%|#3        | 1.88G/13.5G [00:18<02:20, 82.7Mbyte/s]\n",
      "Writing:  15%|#4        | 1.97G/13.5G [00:19<02:10, 88.0Mbyte/s]\n",
      "Writing:  15%|#5        | 2.06G/13.5G [00:20<02:02, 92.9Mbyte/s]\n",
      "Writing:  16%|#5        | 2.15G/13.5G [00:20<01:58, 95.4Mbyte/s]\n",
      "Writing:  16%|#6        | 2.19G/13.5G [00:21<02:03, 91.3Mbyte/s]\n",
      "Writing:  16%|#6        | 2.22G/13.5G [00:21<02:01, 92.9Mbyte/s]\n",
      "Writing:  17%|#6        | 2.25G/13.5G [00:22<02:09, 86.9Mbyte/s]\n",
      "Writing:  17%|#6        | 2.29G/13.5G [00:22<02:04, 90.0Mbyte/s]\n",
      "Writing:  17%|#7        | 2.32G/13.5G [00:22<02:07, 87.7Mbyte/s]\n",
      "Writing:  17%|#7        | 2.35G/13.5G [00:23<02:03, 90.3Mbyte/s]\n",
      "Writing:  18%|#7        | 2.39G/13.5G [00:23<02:11, 84.3Mbyte/s]\n",
      "Writing:  18%|#7        | 2.42G/13.5G [00:24<02:04, 88.7Mbyte/s]\n",
      "Writing:  19%|#8        | 2.51G/13.5G [00:26<03:24, 53.6Mbyte/s]\n",
      "Writing:  19%|#9        | 2.60G/13.5G [00:27<02:43, 66.5Mbyte/s]\n",
      "Writing:  20%|#9        | 2.69G/13.5G [00:28<02:21, 76.1Mbyte/s]\n",
      "Writing:  20%|##        | 2.72G/13.5G [00:28<02:21, 76.0Mbyte/s]\n",
      "Writing:  20%|##        | 2.76G/13.5G [00:29<02:15, 79.2Mbyte/s]\n",
      "Writing:  21%|##        | 2.79G/13.5G [00:29<02:17, 77.8Mbyte/s]\n",
      "Writing:  21%|##        | 2.83G/13.5G [00:29<02:09, 82.5Mbyte/s]\n",
      "Writing:  21%|##1       | 2.86G/13.5G [00:30<02:10, 81.5Mbyte/s]\n",
      "Writing:  21%|##1       | 2.89G/13.5G [00:30<02:02, 86.2Mbyte/s]\n",
      "Writing:  22%|##1       | 2.93G/13.5G [00:31<02:07, 82.7Mbyte/s]\n",
      "Writing:  22%|##1       | 2.96G/13.5G [00:31<02:00, 87.0Mbyte/s]\n",
      "Writing:  23%|##2       | 3.05G/13.5G [00:32<01:51, 93.3Mbyte/s]\n",
      "Writing:  23%|##3       | 3.14G/13.5G [00:33<01:46, 96.7Mbyte/s]\n",
      "Writing:  24%|##3       | 3.23G/13.5G [00:34<01:43, 99.4Mbyte/s]\n",
      "Writing:  25%|##4       | 3.32G/13.5G [00:34<01:43, 98.4Mbyte/s]\n",
      "Writing:  25%|##5       | 3.41G/13.5G [00:35<01:41, 98.9Mbyte/s]\n",
      "Writing:  26%|##5       | 3.50G/13.5G [00:36<01:40, 99.0Mbyte/s]\n",
      "Writing:  26%|##6       | 3.53G/13.5G [00:37<01:45, 94.2Mbyte/s]\n",
      "Writing:  26%|##6       | 3.57G/13.5G [00:37<01:47, 92.3Mbyte/s]\n",
      "Writing:  27%|##6       | 3.60G/13.5G [00:38<01:53, 86.9Mbyte/s]\n",
      "Writing:  27%|##6       | 3.63G/13.5G [00:38<01:49, 89.8Mbyte/s]\n",
      "Writing:  28%|##7       | 3.72G/13.5G [00:39<01:41, 95.6Mbyte/s]\n",
      "Writing:  28%|##8       | 3.81G/13.5G [00:40<01:39, 96.9Mbyte/s]\n",
      "Writing:  29%|##8       | 3.91G/13.5G [00:41<01:37, 97.7Mbyte/s]\n",
      "Writing:  29%|##9       | 3.94G/13.5G [00:41<01:43, 92.3Mbyte/s]\n",
      "Writing:  29%|##9       | 3.97G/13.5G [00:41<01:44, 91.3Mbyte/s]\n",
      "Writing:  30%|##9       | 4.01G/13.5G [00:42<02:05, 75.7Mbyte/s]\n",
      "Writing:  30%|##9       | 4.04G/13.5G [00:43<02:32, 61.9Mbyte/s]\n",
      "Writing:  31%|###       | 4.13G/13.5G [00:44<02:18, 67.7Mbyte/s]\n",
      "Writing:  31%|###1      | 4.22G/13.5G [00:45<02:08, 72.2Mbyte/s]\n",
      "Writing:  32%|###1      | 4.31G/13.5G [00:47<02:02, 74.8Mbyte/s]\n",
      "Writing:  32%|###2      | 4.34G/13.5G [00:47<02:03, 73.7Mbyte/s]\n",
      "Writing:  32%|###2      | 4.38G/13.5G [00:47<02:00, 75.5Mbyte/s]\n",
      "Writing:  33%|###2      | 4.41G/13.5G [00:48<02:03, 73.5Mbyte/s]\n",
      "Writing:  33%|###2      | 4.44G/13.5G [00:48<01:57, 77.2Mbyte/s]\n",
      "Writing:  34%|###3      | 4.53G/13.5G [00:49<01:53, 78.6Mbyte/s]\n",
      "Writing:  34%|###4      | 4.62G/13.5G [00:51<01:51, 79.2Mbyte/s]\n",
      "Writing:  35%|###4      | 4.71G/13.5G [00:52<01:50, 79.4Mbyte/s]\n",
      "Writing:  35%|###5      | 4.75G/13.5G [00:52<01:52, 77.7Mbyte/s]\n",
      "Writing:  35%|###5      | 4.78G/13.5G [00:53<01:49, 79.6Mbyte/s]\n",
      "Writing:  36%|###5      | 4.82G/13.5G [00:53<01:56, 74.7Mbyte/s]\n",
      "Writing:  36%|###5      | 4.85G/13.5G [00:53<01:51, 77.6Mbyte/s]\n",
      "Writing:  37%|###6      | 4.94G/13.5G [00:56<02:36, 54.7Mbyte/s]\n",
      "Writing:  37%|###7      | 5.03G/13.5G [00:57<02:06, 66.9Mbyte/s]\n",
      "Writing:  38%|###7      | 5.12G/13.5G [00:57<01:49, 76.4Mbyte/s]\n",
      "Writing:  39%|###8      | 5.21G/13.5G [00:58<01:38, 84.3Mbyte/s]\n",
      "Writing:  39%|###9      | 5.30G/13.5G [00:59<01:37, 83.8Mbyte/s]\n",
      "Writing:  40%|###9      | 5.39G/13.5G [01:01<01:39, 81.5Mbyte/s]\n",
      "Writing:  40%|####      | 5.42G/13.5G [01:01<01:42, 78.9Mbyte/s]\n",
      "Writing:  40%|####      | 5.46G/13.5G [01:01<01:39, 80.8Mbyte/s]\n",
      "Writing:  41%|####      | 5.49G/13.5G [01:02<01:41, 78.4Mbyte/s]\n",
      "Writing:  41%|####      | 5.52G/13.5G [01:02<01:42, 77.4Mbyte/s]\n",
      "Writing:  42%|####1     | 5.61G/13.5G [01:03<01:40, 78.6Mbyte/s]\n",
      "Writing:  42%|####2     | 5.70G/13.5G [01:05<01:43, 75.4Mbyte/s]\n",
      "Writing:  43%|####2     | 5.79G/13.5G [01:06<01:41, 76.0Mbyte/s]\n",
      "Writing:  43%|####3     | 5.83G/13.5G [01:06<01:42, 74.7Mbyte/s]\n",
      "Writing:  43%|####3     | 5.86G/13.5G [01:07<01:40, 75.9Mbyte/s]\n",
      "Writing:  44%|####3     | 5.90G/13.5G [01:07<01:42, 74.2Mbyte/s]\n",
      "Writing:  44%|####3     | 5.93G/13.5G [01:08<01:37, 77.2Mbyte/s]\n",
      "Writing:  45%|####4     | 6.02G/13.5G [01:09<01:50, 67.4Mbyte/s]\n",
      "Writing:  45%|####5     | 6.11G/13.5G [01:10<01:42, 71.8Mbyte/s]\n",
      "Writing:  46%|####6     | 6.20G/13.5G [01:11<01:38, 74.1Mbyte/s]\n",
      "Writing:  46%|####6     | 6.23G/13.5G [01:12<01:40, 72.1Mbyte/s]\n",
      "Writing:  46%|####6     | 6.27G/13.5G [01:12<01:37, 73.8Mbyte/s]\n",
      "Writing:  47%|####6     | 6.30G/13.5G [01:13<01:39, 71.8Mbyte/s]\n",
      "Writing:  47%|####6     | 6.33G/13.5G [01:13<01:37, 73.5Mbyte/s]\n",
      "Writing:  48%|####7     | 6.42G/13.5G [01:14<01:31, 76.7Mbyte/s]\n",
      "Writing:  48%|####8     | 6.51G/13.5G [01:16<01:30, 76.5Mbyte/s]\n",
      "Writing:  49%|####9     | 6.60G/13.5G [01:17<01:28, 77.3Mbyte/s]\n",
      "Writing:  49%|####9     | 6.64G/13.5G [01:17<01:28, 77.0Mbyte/s]\n",
      "Writing:  50%|####9     | 6.67G/13.5G [01:18<01:27, 78.2Mbyte/s]\n",
      "Writing:  50%|####9     | 6.71G/13.5G [01:18<01:30, 74.9Mbyte/s]\n",
      "Writing:  50%|####9     | 6.74G/13.5G [01:19<01:25, 78.6Mbyte/s]\n",
      "Writing:  51%|#####     | 6.83G/13.5G [01:20<01:31, 73.0Mbyte/s]\n",
      "Writing:  51%|#####1    | 6.92G/13.5G [01:21<01:27, 75.1Mbyte/s]\n",
      "Writing:  52%|#####2    | 7.01G/13.5G [01:22<01:24, 76.2Mbyte/s]\n",
      "Writing:  52%|#####2    | 7.04G/13.5G [01:23<01:25, 75.4Mbyte/s]\n",
      "Writing:  53%|#####2    | 7.08G/13.5G [01:23<01:25, 74.9Mbyte/s]\n",
      "Writing:  53%|#####2    | 7.11G/13.5G [01:24<01:26, 73.5Mbyte/s]\n",
      "Writing:  53%|#####3    | 7.14G/13.5G [01:24<01:23, 76.3Mbyte/s]\n",
      "Writing:  53%|#####3    | 7.18G/13.5G [01:25<01:31, 69.0Mbyte/s]\n",
      "Writing:  54%|#####3    | 7.21G/13.5G [01:25<01:25, 73.2Mbyte/s]\n",
      "Writing:  54%|#####3    | 7.24G/13.5G [01:26<01:34, 65.7Mbyte/s]\n",
      "Writing:  54%|#####3    | 7.28G/13.5G [01:26<01:31, 67.5Mbyte/s]\n",
      "Writing:  55%|#####4    | 7.37G/13.5G [01:28<02:02, 49.9Mbyte/s]\n",
      "Writing:  55%|#####5    | 7.46G/13.5G [01:29<01:34, 63.6Mbyte/s]\n",
      "Writing:  56%|#####6    | 7.55G/13.5G [01:30<01:19, 74.4Mbyte/s]\n",
      "Writing:  57%|#####6    | 7.64G/13.5G [01:31<01:13, 79.4Mbyte/s]\n",
      "Writing:  57%|#####7    | 7.73G/13.5G [01:32<01:11, 79.9Mbyte/s]\n",
      "Writing:  58%|#####8    | 7.82G/13.5G [01:33<01:12, 78.4Mbyte/s]\n",
      "Writing:  58%|#####8    | 7.85G/13.5G [01:34<01:14, 75.6Mbyte/s]\n",
      "Writing:  59%|#####8    | 7.89G/13.5G [01:34<01:11, 77.7Mbyte/s]\n",
      "Writing:  59%|#####8    | 7.92G/13.5G [01:35<01:14, 75.0Mbyte/s]\n",
      "Writing:  59%|#####9    | 7.95G/13.5G [01:35<01:11, 77.3Mbyte/s]\n",
      "Writing:  60%|#####9    | 8.04G/13.5G [01:36<01:07, 80.6Mbyte/s]\n",
      "Writing:  60%|######    | 8.13G/13.5G [01:38<01:08, 77.6Mbyte/s]\n",
      "Writing:  61%|######1   | 8.22G/13.5G [01:39<01:08, 77.2Mbyte/s]\n",
      "Writing:  61%|######1   | 8.26G/13.5G [01:39<01:11, 73.0Mbyte/s]\n",
      "Writing:  62%|######1   | 8.29G/13.5G [01:40<01:09, 75.2Mbyte/s]\n",
      "Writing:  62%|######1   | 8.32G/13.5G [01:40<01:08, 74.8Mbyte/s]\n",
      "Writing:  62%|######2   | 8.36G/13.5G [01:41<01:15, 67.4Mbyte/s]\n",
      "Writing:  63%|######2   | 8.45G/13.5G [01:42<01:11, 70.4Mbyte/s]\n",
      "Writing:  63%|######3   | 8.54G/13.5G [01:43<01:05, 75.3Mbyte/s]\n",
      "Writing:  64%|######4   | 8.63G/13.5G [01:44<01:03, 76.1Mbyte/s]\n",
      "Writing:  64%|######4   | 8.66G/13.5G [01:45<01:03, 75.7Mbyte/s]\n",
      "Writing:  65%|######4   | 8.70G/13.5G [01:45<01:02, 76.4Mbyte/s]\n",
      "Writing:  65%|######4   | 8.73G/13.5G [01:46<01:05, 72.7Mbyte/s]\n",
      "Writing:  65%|######5   | 8.76G/13.5G [01:46<01:11, 66.3Mbyte/s]\n",
      "Writing:  66%|######5   | 8.85G/13.5G [01:48<01:05, 70.1Mbyte/s]\n",
      "Writing:  66%|######6   | 8.94G/13.5G [01:49<01:01, 73.4Mbyte/s]\n",
      "Writing:  67%|######7   | 9.03G/13.5G [01:50<00:58, 75.7Mbyte/s]\n",
      "Writing:  67%|######7   | 9.07G/13.5G [01:50<01:00, 73.0Mbyte/s]\n",
      "Writing:  68%|######7   | 9.10G/13.5G [01:51<00:58, 75.2Mbyte/s]\n",
      "Writing:  68%|######7   | 9.13G/13.5G [01:51<00:59, 72.9Mbyte/s]\n",
      "Writing:  68%|######8   | 9.17G/13.5G [01:52<01:02, 68.9Mbyte/s]\n",
      "Writing:  69%|######8   | 9.26G/13.5G [01:53<00:56, 74.5Mbyte/s]\n",
      "Writing:  69%|######9   | 9.35G/13.5G [01:54<00:53, 77.2Mbyte/s]\n",
      "Writing:  70%|#######   | 9.44G/13.5G [01:55<00:52, 77.0Mbyte/s]\n",
      "Writing:  70%|#######   | 9.47G/13.5G [01:56<00:53, 75.1Mbyte/s]\n",
      "Writing:  71%|#######   | 9.51G/13.5G [01:56<00:51, 76.8Mbyte/s]\n",
      "Writing:  71%|#######   | 9.54G/13.5G [01:57<00:53, 73.6Mbyte/s]\n",
      "Writing:  71%|#######1  | 9.57G/13.5G [01:57<00:54, 71.3Mbyte/s]\n",
      "Writing:  71%|#######1  | 9.61G/13.5G [01:58<00:54, 71.7Mbyte/s]\n",
      "Writing:  72%|#######1  | 9.64G/13.5G [01:58<00:51, 74.4Mbyte/s]\n",
      "Writing:  72%|#######1  | 9.67G/13.5G [01:59<00:55, 68.4Mbyte/s]\n",
      "Writing:  72%|#######2  | 9.71G/13.5G [01:59<00:51, 73.9Mbyte/s]\n",
      "Writing:  73%|#######2  | 9.80G/13.5G [02:01<01:12, 51.1Mbyte/s]\n",
      "Writing:  73%|#######3  | 9.89G/13.5G [02:02<00:56, 63.9Mbyte/s]\n",
      "Writing:  74%|#######4  | 9.98G/13.5G [02:03<00:48, 72.9Mbyte/s]\n",
      "Writing:  75%|#######4  | 10.1G/13.5G [02:04<00:45, 75.2Mbyte/s]\n",
      "Writing:  75%|#######5  | 10.2G/13.5G [02:05<00:43, 76.2Mbyte/s]\n",
      "Writing:  76%|#######6  | 10.2G/13.5G [02:06<00:41, 77.7Mbyte/s]\n",
      "Writing:  76%|#######6  | 10.3G/13.5G [02:07<00:41, 76.3Mbyte/s]\n",
      "Writing:  77%|#######6  | 10.3G/13.5G [02:07<00:40, 78.2Mbyte/s]\n",
      "Writing:  77%|#######6  | 10.3G/13.5G [02:08<00:45, 68.4Mbyte/s]\n",
      "Writing:  77%|#######7  | 10.4G/13.5G [02:09<00:49, 62.4Mbyte/s]\n",
      "Writing:  78%|#######7  | 10.5G/13.5G [02:10<00:43, 68.6Mbyte/s]\n",
      "Writing:  78%|#######8  | 10.6G/13.5G [02:11<00:40, 71.5Mbyte/s]\n",
      "Writing:  79%|#######9  | 10.7G/13.5G [02:12<00:38, 74.3Mbyte/s]\n",
      "Writing:  79%|#######9  | 10.7G/13.5G [02:13<00:37, 73.7Mbyte/s]\n",
      "Writing:  80%|#######9  | 10.7G/13.5G [02:13<00:36, 75.8Mbyte/s]\n",
      "Writing:  80%|#######9  | 10.8G/13.5G [02:14<00:36, 74.7Mbyte/s]\n",
      "Writing:  80%|########  | 10.8G/13.5G [02:14<00:35, 76.7Mbyte/s]\n",
      "Writing:  81%|########  | 10.9G/13.5G [02:15<00:33, 77.6Mbyte/s]\n",
      "Writing:  81%|########1 | 11.0G/13.5G [02:16<00:32, 78.0Mbyte/s]\n",
      "Writing:  82%|########2 | 11.1G/13.5G [02:17<00:30, 79.4Mbyte/s]\n",
      "Writing:  82%|########2 | 11.1G/13.5G [02:18<00:31, 76.6Mbyte/s]\n",
      "Writing:  83%|########2 | 11.1G/13.5G [02:18<00:30, 77.8Mbyte/s]\n",
      "Writing:  83%|########2 | 11.2G/13.5G [02:19<00:30, 75.6Mbyte/s]\n",
      "Writing:  83%|########3 | 11.2G/13.5G [02:19<00:28, 78.8Mbyte/s]\n",
      "Writing:  84%|########3 | 11.3G/13.5G [02:20<00:27, 78.7Mbyte/s]\n",
      "Writing:  84%|########4 | 11.4G/13.5G [02:21<00:26, 79.4Mbyte/s]\n",
      "Writing:  85%|########5 | 11.5G/13.5G [02:23<00:25, 80.6Mbyte/s]\n",
      "Writing:  85%|########5 | 11.5G/13.5G [02:23<00:25, 76.7Mbyte/s]\n",
      "Writing:  86%|########5 | 11.5G/13.5G [02:23<00:24, 78.5Mbyte/s]\n",
      "Writing:  86%|########5 | 11.6G/13.5G [02:24<00:25, 76.6Mbyte/s]\n",
      "Writing:  86%|########6 | 11.6G/13.5G [02:25<00:27, 68.1Mbyte/s]\n",
      "Writing:  87%|########6 | 11.7G/13.5G [02:26<00:24, 74.2Mbyte/s]\n",
      "Writing:  87%|########7 | 11.8G/13.5G [02:27<00:22, 74.9Mbyte/s]\n",
      "Writing:  88%|########8 | 11.9G/13.5G [02:28<00:21, 75.3Mbyte/s]\n",
      "Writing:  88%|########8 | 11.9G/13.5G [02:29<00:22, 69.2Mbyte/s]\n",
      "Writing:  89%|########8 | 11.9G/13.5G [02:29<00:21, 71.8Mbyte/s]\n",
      "Writing:  89%|########8 | 12.0G/13.5G [02:30<00:21, 71.5Mbyte/s]\n",
      "Writing:  89%|########9 | 12.0G/13.5G [02:30<00:19, 74.1Mbyte/s]\n",
      "Writing:  89%|########9 | 12.0G/13.5G [02:30<00:19, 74.0Mbyte/s]\n",
      "Writing:  90%|########9 | 12.1G/13.5G [02:31<00:19, 72.5Mbyte/s]\n",
      "Writing:  90%|########9 | 12.1G/13.5G [02:31<00:18, 73.3Mbyte/s]\n",
      "Writing:  90%|######### | 12.1G/13.5G [02:32<00:17, 78.9Mbyte/s]\n",
      "Writing:  92%|#########1| 12.4G/13.5G [02:36<00:16, 66.4Mbyte/s]\n",
      "Writing:  93%|#########2| 12.5G/13.5G [02:37<00:14, 70.6Mbyte/s]\n",
      "Writing:  93%|#########3| 12.6G/13.5G [02:38<00:12, 71.8Mbyte/s]\n",
      "Writing:  94%|#########3| 12.7G/13.5G [02:39<00:10, 75.0Mbyte/s]\n",
      "Writing:  95%|#########4| 12.8G/13.5G [02:40<00:09, 76.7Mbyte/s]\n",
      "Writing:  95%|#########5| 12.8G/13.5G [02:42<00:08, 77.0Mbyte/s]\n",
      "Writing:  96%|#########6| 12.9G/13.5G [02:43<00:06, 77.6Mbyte/s]\n",
      "Writing:  96%|#########6| 13.0G/13.5G [02:43<00:06, 76.3Mbyte/s]\n",
      "Writing:  96%|#########6| 13.0G/13.5G [02:44<00:06, 77.4Mbyte/s]\n",
      "Writing:  97%|#########6| 13.0G/13.5G [02:44<00:05, 74.9Mbyte/s]\n",
      "Writing:  97%|#########6| 13.1G/13.5G [02:44<00:05, 77.8Mbyte/s]\n",
      "Writing:  98%|#########7| 13.2G/13.5G [02:46<00:04, 69.8Mbyte/s]\n",
      "Writing:  98%|#########8| 13.3G/13.5G [02:47<00:03, 73.9Mbyte/s]\n",
      "Writing:  99%|#########9| 13.3G/13.5G [02:48<00:01, 74.6Mbyte/s]\n",
      "Writing:  99%|#########9| 13.4G/13.5G [02:49<00:01, 72.4Mbyte/s]\n",
      "Writing: 100%|#########9| 13.4G/13.5G [02:49<00:00, 74.3Mbyte/s]\n",
      "Writing: 100%|#########9| 13.4G/13.5G [02:50<00:00, 72.4Mbyte/s]\n",
      "Writing: 100%|#########9| 13.5G/13.5G [02:50<00:00, 76.0Mbyte/s]\n",
      "Writing: 100%|##########| 13.5G/13.5G [02:51<00:00, 78.7Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to llama2_7b_chat.gguf\n"
     ]
    }
   ],
   "source": [
    "!python Documents/llama.cpp/convert_hf_to_gguf.py ./documents/saved_model --outfile llama2_7b_chat.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb34436d-5f1d-464a-9b01-ce011d8b96bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from C:\\Users\\acer\\Documents\\llama2_7b_chat.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = saved_model\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = saved_model\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size = 12853.02 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    70.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'general.name': 'saved_model', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '4096', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '1', 'llama.attention.head_count_kv': '32', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '32000', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n",
      "\n",
      "llama_print_timings:        load time =    4819.90 ms\n",
      "llama_print_timings:      sample time =       7.69 ms /    16 runs   (    0.48 ms per token,  2081.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4819.74 ms /    15 tokens (  321.32 ms per token,     3.11 tokens per second)\n",
      "llama_print_timings:        eval time =   12522.04 ms /    15 runs   (  834.80 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time =   17366.58 ms /    30 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      "Q: tell me a story about a brave knight A:  Sure! Here is a story about a brave knight named Sir Edward:\n",
      "\n",
      "Reference:\n",
      "A brave knight fought valiantly against the dragon.\n",
      "\n",
      "BLEU Score: 3.197383344450448\n",
      "Inference Time: 17.37819766998291 seconds\n",
      "Memory Usage Before Inference: 538.19 MB\n",
      "Memory Usage After Inference: 13146.93 MB\n",
      "Memory Used: 12608.74 MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from llama_cpp import Llama\n",
    "import sacrebleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the Llama model with the correct path to your model\n",
    "llm = Llama(\n",
    "    model_path=r\"C:\\Users\\acer\\Documents\\llama2_7b_chat.gguf\"\n",
    "    # You can uncomment and adjust parameters like n_gpu_layers, seed, or n_ctx here if needed\n",
    ")\n",
    "\n",
    "# Define your prompt for evaluation\n",
    "prompt = \"Q: tell me a story about a brave knight A: \"\n",
    "\n",
    "# Measure memory usage before inference\n",
    "memory_before = get_memory_usage()\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "# Generate model output\n",
    "output = llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=100,  # Generate up to 100 tokens\n",
    "    stop=[\"Q:\", \"\\n\"],  # Stop generating before the model generates a new question or newline\n",
    "    echo=True,  # Echo the prompt back in the output\n",
    "    temperature=0.7,  # Adjust temperature for randomness (0.7 is moderately random)\n",
    "    top_p=0.9  # Adjust top_p for diversity (0.9 means considering the top 90% of probability mass)\n",
    ")\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "# Measure memory usage after inference\n",
    "memory_after = get_memory_usage()\n",
    "# Calculate inference time\n",
    "inference_time = end_time - start_time\n",
    "memory_used = memory_after - memory_before\n",
    "# Extract generated text from the output\n",
    "generated_output = output['choices'][0]['text']\n",
    "\n",
    "# Reference text (ground truth) for evaluation\n",
    "reference = \"A brave knight fought valiantly against the dragon.\"\n",
    "\n",
    "# Tokenize generated output and reference\n",
    "generated_tokens = word_tokenize(generated_output)\n",
    "reference_tokens = word_tokenize(reference)\n",
    "\n",
    "# Compute BLEU score using SacreBLEU\n",
    "bleu = sacrebleu.corpus_bleu([generated_output], [[reference]])\n",
    "\n",
    "# Print the generated output and BLEU score\n",
    "print(f\"Generated Output:\\n{generated_output}\\n\")\n",
    "print(f\"Reference:\\n{reference}\\n\")\n",
    "print(f\"BLEU Score: {bleu.score}\")\n",
    "print(f\"Inference Time: {inference_time} seconds\")\n",
    "# Print memory usage\n",
    "print(f\"Memory Usage Before Inference: {memory_before:.2f} MB\")\n",
    "print(f\"Memory Usage After Inference: {memory_after:.2f} MB\")\n",
    "print(f\"Memory Used: {memory_used:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3339b0-f096-4b5c-9b4a-37ed95a9bef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
